\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{parskip}
\usepackage{mathpazo}

\usepackage{verbatim}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{courier}
\usepackage{minted}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[breaklinks]{hyperref}
\hypersetup{hidelinks}
%\usepackage[hyphens]{url}
\usepackage[numbers]{natbib}
\usepackage{breakurl}

\lstset{breaklines=true}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{tgpagella}

\usepackage{xcolor}
\usepackage{booktabs} % for \toprule etc.
\usepackage{colortbl} % for \arrayrulecolor

\usepackage{caption}
%\DeclareCaptionFont{white}{\color{white}}
%\DeclareCaptionFormat{listing}{\colorbox[cmyk]{0.43, 0.35, 0.35,0.01}{\parbox{\textwidth}{#1 #2 #3}}}
%\captionsetup[listing]{format=listing,labelfont=white,textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize}}


\input{epsf}                            % to allow postscript inclusions
% On thor and CUS read top of file:
%     /opt/TeX/lib/texmf/tex/dvips/epsf.sty
% On CL machines read:
%     /usr/lib/tex/macros/dvips/epsf.tex

\pgfplotsset{width=15cm}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\usetikzlibrary{arrows,positioning} 
\tikzset{
    %Define standard arrow tip
    >=stealth,
    %Define style for boxes
    punkt/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           text width=6em,
           minimum height=2em,
           text centered},
    % Define arrow style
    pil/.style={
           ->,
           very thick,
           shorten <=2pt,
           shorten >=2pt,}
}

%\setlength{\parskip}{0pt}

\begin{document}

\bibliographystyle{plain}

\newcommand{\name}{Joseph Seaton}
\newcommand{\college}{Fitzwilliam College}
\newcommand{\ptitle}{Shader Compositor}

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf \name}

\vspace*{60mm}
\begin{center}
\Huge
{\bf \ptitle} \\
\vspace*{5mm}
Computer Science Tripos, Part II\\
\vspace*{5mm}
\college \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures



\section*{Proforma}

{
\begin{tabular}{@{\hspace{0em}}ll}
Name:               & \bf \name    \\
College:            & \bf \college \\
Project Title:      & \bf \ptitle  \\
Examination:        & \bf Computer Science Part II, 2012\\
Word Count:         & \bf About 9500 \\
Project Originator: & Christian Richardt                    \\
Supervisor:         & Christian Richardt                    \\ 
\end{tabular}
}
% detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w
\stepcounter{footnote}


\subsection*{Original aims of the project}
The aim of this project was to produce a novel user interface for easy composition and testing of a certain class of graphical effects known as shaders. The user interface was to be highly interactive and allow easy modification of the code for the effects themselves, the manner in which they are composed and, via graphical means, their associated parameters. Previews of these changes were to be provided as quickly as possible; therefore the project also included a number of optimisations.

\subsection*{Work completed}
The core of the project has been completed and works satisfactorily. Shaders can be created and a graph of shaders can be specified using JavaScript, the result of which is shown in-browser. An interface for modifying shader parameters is correctly generated for most types of parameter that have been tested, and some annotations can be correctly parsed. A number of optimisations have been applied, including elimination of unnecessary shader recompilation and pipeline re-generation. The project is also now capable of reusing framebuffer objects between pipeline changes where possible.

\subsection*{Special difficulties}
None.
 
\newpage
\setlength{\parskip}{9pt}
\section*{Declaration}

I, \name \ of \college, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed}

\medskip
\leftline{Date }

\cleardoublepage

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

%\setcounter{page}{1}
%\pagenumbering{arabic}
%\pagestyle{headings}

\chapter{Introduction}
My project provides a novel user interface for easily composing multiple graphical effects known as shaders together, in which individual shader parameters can be easily modified and the results viewed immediately. Furthermore the interface is sufficiently optimised to provide a low round-trip time between making a change and seeing the effect of this change. I have successfully implemented the core of my project, as well as a number of proposed extensions and a few optimisations not initially proposed.

\section{Motivation}
Working with raw OpenGL is undeniably messy. OpenGL is a verbose and complicated API. But, when developing shaders, developers tend not to be interested in writing large amounts of OpenGL API code just to tweak shader parameters or forward the texture output of one shader to the input of another. But this is exactly what developers have to do. Given that many graphics applications are written in compiled languages like C and C++, this significantly increases the length of a development cycle. As a large part of shader development constitutes small tweaks to (often subjectively) improve output, this is not ideal.

For students learning about shaders, the OpenGL API is often an intimidating and confusing mess that gets in the way of understanding a fundamentally quite straightforward concept. Listing~\ref{ugly} shows some example OpenGL calls.

This project aims to provide a way of developing shaders that is both useful for developers, and may be used as a tool for students learning about shaders for the first time.
\section{A brief introduction to shaders}
\label{brief}
In their modern incarnation, OpenGL shaders are quite a simple concept to understand at a high level. A shader is a simple program --- `simple' referring to certain constraints we shall ignore for now --- written in a dataflow style. Such programs take in one item of data at a time, for example a vertex, and output another item of data --- the new vertex location, or a pixel value.

In a modern OpenGL implementation, there are two commonly used types of shader (and a few more which will not be discussed). These are vertex shaders and fragment shaders. Vertex shaders operate on individual vertices, transforming the position and also attaching information such as a colour. Fragment shaders roughly speaking generate pixel values. Information can be passed from vertex shaders to fragment shaders using \texttt{varying} parameters, the values of which are interpolated from those of the surrounding vertices. Vertex shaders also accept \texttt{attribute} parameters, specifying per-vertex information, and both accept \texttt{uniform} parameters, which specify vertex-independent information. This last type of parameter is of most interest in this project.

\subsection{A look at some OpenGL calls}
As discussed above, the use of shaders sounds simple. However, as we can see from Listing~\ref{ugly}, a short excerpt of the OpenGL calls from an actual C++ OpenGL program\citep{samples}, the paraphernalia of mostly boilerplate OpenGL calls required for compiling shader programs, specifying parameters and so on can result in a very large codebase for even the simplest program.

\begin{listing}
\begin{minted}[mathescape]{cpp}
glBindFramebuffer(GL_FRAMEBUFFER, 0);
glUseProgram(ProgramName[IMAGE_2D]);
glUniformMatrix4fv(UniformMVP[IMAGE_2D], 1, GL_FALSE, 
                   &MVP[0][0]);
glUniform1i(UniformDiffuse, 0);

glActiveTexture(GL_TEXTURE0);
glBindTexture(GL_TEXTURE_2D_ARRAY, TextureColorbufferName);
glBindSampler(0, SamplerName);

glBindVertexArray(VertexArrayName[IMAGE_2D]);
glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 
             BufferName[BUFFER_ELEMENT]);
\end{minted}
\caption{Example WebGL calls.\label{ugly}}
\end{listing}

\cleardoublepage
\chapter{Preparation}
Before actually embarking on the project, there were a few decisions to be made, as discussed in this chapter. Firstly, I had to decide upon the languages and any supporting libraries. This necessitated some degree of background reading. Then, I identified the individual components of the project, and developed an appropriate schedule.

\section{Requirements analysis}
Before beginning the project, it was important to establish the general architecture. In Table~\ref{goals} we can see a summary of the main objectives of the project.

\begin{table}[h!]
\centering
\begin{tabular}{p{9cm}l@{\hspace{0.25em}}}
\toprule
\textbf{Goal} & \textbf{Priority} \\
\midrule\arrayrulecolor{lightgray}
Simple interface & High \\\midrule
Parameter detection and presentation & High \\\midrule
Pipeline generation & High \\\midrule
Pipeline rendering & High \\\midrule
Syntax highlighting & Medium \\\midrule
Optimisations & Medium \\\midrule
Annotations & Medium \\\midrule
Additional UI work & Low \\\midrule
Animation & Low \\\arrayrulecolor{black}\bottomrule
\end{tabular}
\caption{Main goals.\label{goals}}
\end{table}


In order to properly plan the development of the project, it was necessary to construct a graph of the dependencies between parts of the project. Priority could then be given to those parts of the project that were necessary for other parts of the core. Figure~\ref{depend} shows the general dependencies between larger parts of the project. The GLSL parser (see Implementation) and the user interface are initially blocking multiple tasks, and as such were scheduled first, followed by pipeline generation and rendering, since these are high priority. Syntax highlighting and annotations were scheduled next, since these have the least remaining dependencies, followed by optimisations. The low priority tasks were scheduled last since they could be skipped.

\begin{figure}
\centering
\begin{tikzpicture}[->,node distance=1cm]
 \node[punkt] (parser) {Parser};
 \node[punkt, right=of parser] (param) {Parameter detection};
 \node[punkt, right=of param] (param-pres) {Parameter presentation};
 \node[punkt, below=of param-pres] (ui) {UI};
 \node[punkt, right=of param-pres] (opt) {Optimisations};
 \node[punkt, below=of parser] (annot) {Annotations};
\node[punkt, below=of ui] (syn) {Syntax highlighting};
\node[punkt, left=of ui] (more-ui) {Additional UI};
\node[punkt, above=of param-pres] (anim) {Animation};

\path[pil]   (parser.east)  edge              node {} (param.west)
        (param) edge              node {} (param-pres)
        (ui) edge              node {} (param-pres)
        (param-pres) edge              node {} (opt)
        (parser) edge              node {} (annot)
      (ui)  edge   node {} (syn)
(ui)  edge   node {} (more-ui)
(annot)  edge   node {} (more-ui)
(param-pres)  edge   node {} (anim)
;
\end{tikzpicture}
\caption{Project dependencies.\label{depend}}
\end{figure}

\section{Preparatory learning}
\subsection{A short introduction to OpenGL}
OpenGL is a hardware-, operating-system and vendor-independent graphics API. It has a stateful, client-server architecture, in which the client, the OpenGL library sends commands --- OpenGL API calls --- to the server, the actual OpenGL driver. Commands set up a state, and then pass through data, which is affected by said state. These commands enter a pipeline, a very simplified version of which is shown in Figure~\ref{opengl-pipe}. The vertex and fragment shaders discussed in Section~\ref{brief} are contained within the per-vertex and per-fragment operation blocks respectively. 

The per-vertex operations are, in modern OpenGL, mostly dealt with by the vertex shader. This stage is concerned with transformation of vertices. Typically this involves transformation of the vertex position from local coordinates through world coordinates to camera coordinates. Other common operations include generating texture coordinates, and per-vertex lighting.

The rasterisation stage converts performs necessary operations on vertices like clipping, projection and backface culling. It is here that per-fragment values are generated: that is, values like colour for each potential on-screen (or at least, on-framebuffer) pixel are interpolated based on the surrounding vertices. These interpolated values are passed to the fragment shader as \texttt{varying} parameters.

The per-fragment operations are again mostly covered by the fragment shader in OpenGL. In this stage, the fragment shader runs on each individual fragment. Typically this stage is used for texturing, but it is also commonly used for more sophisticated forms of lighting. A common trick is to use interpolated normals based on the vertex normals to provide more realistic specular lighting, or use a texture as a normal map to provide bump-mapping.

Of particular concern to for this project is the final stage in this pipeline, in which fragments are written to a framebuffer. A framebuffer object or FBO contains several buffers including a colour buffer, which may accessed as a texture by other stages. It is this facility that allows multiple vertex/fragment shader pairs to be connected together in arbitrary ways, as this project explores.

The OpenGL pipeline is designed in such a way as to make extracting parallelism very easy. Each shader may be run for many different vertices or fragments simultaneously, with no danger of cross-dependencies within a given mesh. Modern graphics processors or GPUs are designed with this in mind, and feature many `stream processors', each capable of executing a given shader for many data at once. This massive parallelism is crucial to enabling modern, high-performance graphics. However, these stream processors are limited in that they can only deal with programs of a certain size, which must lack certain features like unbounded recursion. This limited size is a part of why shader composition, as facilitated by this project, is useful, since it allows us to connect many smaller shader programs.
\begin{figure}
\centering
\begin{tikzpicture}[->,node distance=1cm, auto,]
\node[punkt] (cmd) {Commands};
\node[punkt, below=of cmd] (eval) {Evaluator};
\node[punkt, below=of eval] (vert) {Per-vertex operations};
\node[punkt, below=of vert] (rast) {Rasterisation};
\node[punkt, below=of rast] (frag) {Per-fragment operations};
\node[punkt, below=of frag] (fb) {Framebuffer};

\node[punkt, right=of eval] (disp) {Display list};
\node[punkt, right=of vert] (pix) {Pixel operations};
\node[punkt, right=of pix] (tex) {Texture memory};

\path[pil]   (cmd)  edge              node {} (eval)
        (eval) edge              node {} (vert)
        (vert) edge              node {} (rast)
        (rast) edge              node {} (frag)
        (frag) edge              node {} (fb)
      (cmd)  edge  [bend left] node {} (disp)
      (disp) edge              node {} (eval)
      (disp) edge              node {} (pix)
      (disp) edge              node {} (eval)
      (pix)  edge  [bend left] node {} (rast.east)
      (pix)  edge              node {} (tex)
      (fb.east)  edge [bend right] node {} (pix)
;


\end{tikzpicture}
\caption{Simplified OpenGL pipeline.\label{opengl-pipe}}
\end{figure}

\section{Programming language and libraries}

\subsection*{WebGL}
WebGL is a very recently developed API allowing web-based applications access to OpenGL ES contexts, via JavaScript \citep{webgl-spec}. I decided that the use of web-based technologies would help lower the barrier for use of this project, and therefore shader development in general, since a user needs only visit the correct page using a modern web browser. Using WebGL has the added advantage of requiring less familiarisation given my prior experience with JavaScript. WebGL uses essentially the same API as OpenGL ES for C \citep{webgl-spec}.

\subsection*{OpenGL and GLSL}
GLSL is the only choice of shading language for WebGL, and is the standard high-level shading language of OpenGL. Syntactically it is very similar to C, and is intended to be written by humans. Alternatively for a project not using WebGL, I could have chosen to use Cg, a language with similar goals but the ability to output OpenGL and DirectX compatiable shader programs. However WebGL does not support Cg and does not appear to be as well known. 

Prior to beginning this project, I had very little knowledge of OpenGL barring a small amount of personal experience writing toy programs in my spare time. As such, I needed to gain a better understanding of OpenGL, and some knowledge of GLSL before I could begin this project. For this purpose, I obtained a copy of the famous `Red Book' \citep{redbook}. I also read through the GLSL ES specification provided by the Khronos Group \citep{glsl-spec}, which discusses the syntax of GLSL in detail.

\subsection*{JavaScript}
JavaScript is the natural choice for any client-side web development work, and it is currently unavoidable that any WebGL project will have to at least output JavaScript at some point. While JavaScript has a less than stellar image due to implementation inconsistencies between browsers, the language itself is quite sophisticated, featuring first-class functions, closures, and a prototype-based object model.

While, like many people, I already have some experience of JavaScript programming from web development work in the past, I thought it would be wise to gain a broader understanding of JavaScript before embarking on a more complex project such as this one. I found particularly useful {\it Eloquent Javascript }\citep{eloquent} and {\it Higher Order Programming [in JavaScript] }\citep{higher-order}, both of which are unusual among texts on JavaScript in being willing to discuss the more sophisticated functional aspects of JavaScript, and their relation to JavaScript's own unusual prototype-based object system.

\subsection*{jQuery}
jQuery is a general utility library for JavaScript, which provides many helper functions not included in web browsers, and also greatly simplified DOM (Document Object Model) manipulation. In Listing~\ref{jq} we see the difference in concision with and without jQuery for some simple common tasks.

\begin{listing}[H]
\begin{minted}[fontsize=\small]{javascript}
// JavaScript
[].forEach.call(document.querySelectorAll("a"), function(el) {
  el.addEventListener("click", function() {
    ...
  });
});

// jQuery
$("a").clickfunction() {
  ...
})

\end{minted}
%$
\caption{Comparison of plain JavaScript with jQuery for adding a 'click' event handler to all links.\label{jq}}
\end{listing}

\subsection*{Testing framework}
There are many unit testing frameworks available for JavaScript, of varying levels of complexity. Many of these frameworks are designed with very large projects and extensive test suites in mind. For my project, I considered these to be overly complicated to work with. Eventually, I settled on QUnit\citep{qunit}, a very simple unit testing framework that is part of jQuery, and it's port to node.js (a fork of the original code) \citep{qunit-node}.

\subsection*{WebGL toolkit}
Given that OpenGL and therefore WebGL itself can be quite awkward to work with, I decided to use an existing library to abstract away some of the boilerplate code irrelevant to my project. However, there are currently many libraries available claiming to do exactly this, of varying degrees of completeness and varying levels of abstraction. Initially, I settled on three.js \citep{three} on the basis of its popularity and high rate of development. However, it soon became apparent that the level of abstraction provided by three.js actually made the task more difficult, given that interacting with FBOs was non-obvious. After some more searching, I came across \textsc{glow} \citep{glow}, a toolkit specifically designed to make working with shaders simple, but otherwise providing little abstraction.

\subsection*{Editor}
\label{cmirror}
One important component of the project is to provide good GLSL syntax highlighting. There currently exist many web-based editors offering some degree of syntax highlighting, but I chose to use CodeMirror \citep{codemirror}, given its active development and the presence of a modular way to add support for highlighting new grammars. 

\subsection*{jQuery UI}
jQuery UI is a user interface library built on top of jQuery. It provides a number of common, basic widgets including a simple tabbing interface. Using a pre-existing library is especially useful for user interface work in JavaScript, since widgets will have been tested against multiple browsers, a time-consuming and tricky process.

\section{Development environment}
\subsection*{Emacs}
Surprisingly for such an old editor, with the correct extensions, Emacs is very well equipped for modern JavaScript development. While there are other editors available that could claim most or all of the features these extensions add, my previous experience with Emacs makes this a favourable choice. The extensions used add auto-completion, syntax checking, and an inline node.js (see below) console \citep{emacs-js}.

\subsection*{node.js}
node.js is a JavaScript platform built on top of Google's V8 JavaScript engine\citep{v8}. It provides, among other things, a command line interface, including a read, evaluate, print loop (or REPL). This provides a fast, light-weight way to test non-browser-bound code using traditional command line tools, can be easily interfaced with git and cron, and can easily write to files. All of this can be done without requiring messy extra code to interact with a web browser or server-side code to report back statistics.

\subsection*{Google Chrome}
WebGL naturally requires the use of a web browser. I have found that Google Chrome provided the most reliable WebGL implementation on my development machine, which runs Linux on an ATI HD 6850 graphics card. Since {WebGL} is a relatively recent development that has only recently gained browser support, WebGL can still be unstable and buggy in some circumstances --- even using Google Chrome. Chrome also includes a set of 'developer tools' including a JavaScript console 

\subsection*{WebGL inspector}
WebGL inspector is a Google Chrome extension intended to provide inline information on current WebGL contexts, giving easy access to state information including currently allocated textures. While this tool would be very useful for debugging, I found the tool to be insufficiently stable on my operating system (Linux) to use reliably. This meant that most inspection of OpenGL state had to be performed manually.


\subsection*{Version control and backup strategy}
All project files, including the dissertation, were placed under git version control. Git is a distributed version control system, thus it was possible to maintain multiple complete copies of the repository in different locations. Git was then configured to push to multiple remote repositories in different locations: an external harddrive, my PWF account, my SRCF account, my web hosting account, and GitHub \citep{github}. While this could be considered excessive, the use of SSH keys meant that post-setup, this required no extra effort on my part. Since I am conscious that git is capable of history rewriting, I also configured a regular cron job to make regular compressed backups of my current repository using \texttt{git bundle}.

\section{Software development process}
WebGL is still quite a new technology, and as such the libraries surrounding it are not yet mature. Given that I was also not overly familiar with OpenGL at the beginning of the project, I deemed it sensible to apply an iterative software development model, to allow the requirements to be updated after prototyping. 

I decided to use an evolutionary software development model. This model employs rapid prototyping, enabling fast experimentation and evaluation of the feasibility of a given route. The model also incorporates feedback from development into the specification.

Test-driven development was also employed for some parts of the project, most prominently the parser. The use of unit testing enables early, automated detection of regressions, making it easy to spot bugs that might otherwise go unnoticed. Git pre-commit hooks were used to enforce running tests regularly.


\cleardoublepage
\chapter{Implementation}
The project essentially consists of two parts; the backend and the user interface. The main flow of control within the program can be viewed as a pipeline, with each stage depending on results from the previous stage, and passing results to the next stage. The user interface provides new input, and either runs through the entire pipeline in the simple case, or updates individual stages in the more complex case. In either case, changes to earlier stages necessitates updating all later stages. The project is further divided into the following parts:

\begin{enumerate}
\item \textbf{Parser}\\
The parser extracts relevant parameters from a given shader, and their associated types, precisions and arity. Using annotations extra information for the User Interface is also extracted.
\item \textbf{Pipeline specification}\\
The user-specified shader graph is extracted and converted into a usable format, with dummy shaders as presented to the user being converted into actual shader instances.
\item \textbf{Parameter UI generation}\\
An appropriate user interface for specifying parameters for each shader instance in the pipeline is generated based on the pipeline specification and the shader parameters extracted by the parser.
\item \textbf{Pipeline generation}\\
A list of shaders is generated from the shader graph such that the shaders can be rendered in sequence. This creates \textsc{glow} shader objects for each shader and assigns framebuffer objects as necessary.
\item \textbf{Rendering}\\
This stage traverses the pipeline and renders each shader in turn. The resulting pipeline output is then displayed in the user interface.
\item \textbf{User interface}\\
The user interface connects the above stages together in an event-driven manner, and allows the user to input shaders and their connections and parameters. It also provides feedback for errors.
\end{enumerate}

\clearpage
\section{GLSL Parser}
The main job of the parser is to take a shader program and provide an array of parameter names and their corresponding data types, which must be supplied to a shader program for it to run. However the entire GLSL program is parsed, to provide extra information, for example for syntax highlighting. For each parameter, the parser returns an object specifying:
\begin{itemize}
  \item A parameter qualifier: this can be \texttt{uniform}, \texttt{varying} or \texttt{attribute}. We are primarily concerned with \texttt{uniform} parameters since these are exposed to the user
  \item A type: this may be a fundamental type like \texttt{int} or a structure, in which case the type is an object like a parameter object, barring the parameter qualifier
  \item Precision: either high, medium or low. Unused, but could provide error information
  \item Arity: if the parameter is an array, the size of the array (this is required to be constant)
  \item Annotations: these include range and colour annotations
\end{itemize}
All of these other than the type may be left unspecified. The parser could also return information for the syntax highlighter, but this was not implemented. Since WebGL is based on a very specific version of OpenGL, OpenGL ES 2.0, the parser only needs to support GLSL ES 1.0.17 \citep{glsl-spec}\citep{webgl-spec}. This simplifies the construction of the parser, as we need not worry about different GLSL versions.

Since GLSL's grammar is mostly LALR, I deemed it sensible to use a parser generator rather than expend effort writing a parser by hand. Initially I chose to use JS/CC \citep{js-cc}, since it seemed well-documented and quite popular. However, once I had used it to generate an appropriate parser for GLSL, it quickly became apparent that the parser was too inefficient to use for such a complicated grammar. In fact, I was unable to obtain any timing for the parser, as under Google Chrome, even with the simplest inputs, it would fail to produce any output before Chrome itself decided to kill the process. While it may have been possible to obtain timing information, this is clearly unacceptable. 

I then found Jison \citep{jison} --- a JavaScript port of GNU Bison. Bison is a venerable parser generator which is part of the GNU project, but it is only capable of generating parsers in C, C++ and Java.  Jison was able to parse even quite complicated input in a reasonable amount of time, as is discussed in my evaluation. 

As an alternative to using Jison, which is relatively untested compared to Bison, I could have used server-side parsing. However, this would have imposed the need for websites using my project to compile code to run on their servers, and configure their webservers appropriately. Conversely using Jison, no server-side processing is required at all, much reducing the barrier to using my project.

\subsection{Grammar conversion}
The grammar for GLSL is provided for GLSL ES in the specification \citep{glsl-spec}, in Backus–Naur Form (BNF). Since Jison's specification language is roughly based on BNF, the conversion is mostly straightforward as shown in Listing~\ref{grammar-rules}. The additional annotations between curly braces are constructing parameter type objects to be passed upwards. Token specification is likewise mostly straightforward, with the exception of struct-related tokens as discussed below, and integer / floating-point literals which must be represented via regular expressions. Care must also be taken with the ordering of token definitions, in particular the \texttt{IDENTIFIER} token is capable of capturing most keywords and must be placed after them.
\begin{listing}
\begin{minted}[mathescape]{javascript}
//Input: BNF form from specification

type_specifier:
        type_specifier_no_prec
        precision_qualifier type_specifier_no_prec

//Output: Jison version

type_specifier:
        type_specifier_no_prec { 
          $$ = {type:$1}; 
        }
        | precision_qualifier type_specifier_no_prec { 
          $$ = {type:$2,prec:$1}; 
        }
	;
\end{minted}
%$
\caption{Translation of GLSL grammar rules.\label{grammar-rules}}
\end{listing}

\subsection{Preprocessor}
GLSL contains a preprocessor language much like that used in C. It is very restricted and only allows a few simple substitutions and if/else statements, error messages and compiler information that can be ignored according to the specification. It is often used to make decisions depending on the GLSL language version, but since {WebGL} currently only supports one version of GLSL it is not generally used in WebGL programs. Implementation would require a simple initial pass over the shader code before forwarding it to the main parser. Because of the limited utility of the preprocessor, I decided not to implement it in my parser, but this would be an easy extension.

\subsection{Annotations}
\label{annotations}
In addition to parsing raw GLSL, the parser was intended to be able to detect additional annotations to parameters. These annotations would provide extra information to enrich the usability of the parameter UI, for example by specifying the expected range of values taken by a parameter or, in the case of vectors, whether to provide a colour picking interface. In order to keep compatibility with GLSL, I decided to provide these annotations via comments, as shown in Listing~\ref{annotated}. 

According to the GLSL specification \citep{glsl-spec}, comments are not to be handled by the parser, but rather be stripped out by a preprocessor. Therefore annotations are achieved by way of my own preprocessor that translates GLSL into `annotated GLSL' by replacing comments of a certain form with code using novel keywords, an example of which we can see in Listing~\ref{annotated}. Note that the introduced keywords are named to avoid conflicting with likely actual variable names. Other types of comment are simply removed, as required of the preprocessor by the specification. Since the syntax of these annotations is quite simple, this can be done using regular expressions. This annotated GLSL can then be passed to the parser, the grammar of which is modified to accept these annotations. An example of the rule changes can be seen in \ref{rules}.

\begin{listing}
\begin{minted}{glsl}
//Standard GLSL
uniform highp float y; //range 0,100
uniform lowp vec3 c;   //colour

//Annotated GLSL
uniform highp float y __range__annotation 0,100;
uniform lowp vec3 c __colour__annotation;
\end{minted}
\caption{Transformation to annotated GLSL.\label{annotated}}
\end{listing}

\begin{listing}
\begin{minted}[fontsize=\small]{javascript}
//Original rule
init_declarator_list:
        single_declaration
        | /* omitted */
	;

//New rules
init_declarator_list_unannotated:
        single_declaration
        | /* omitted */
	;

init_declarator_list:
	init_declarator_list_unannotated
	| init_declarator_list_unannotated COLOUR { 
          for (i in $$) { $$[i].colour = true; } 
        }
	| init_declarator_list_unannotated RANGE INTCONSTANT 
        COMMA INTCONSTANT { 
          for (i in $$) { $$[i].range = {bottom:$3,top:$5}; } 
        }
	;
\end{minted}
\caption{Modifications to grammar rules.\label{rules}}
\end{listing}

\subsection{Structure parsing}
\label{struct-parse}
The GLSL Specification \citep{glsl-spec} allows a shader program to define and use new types of structures. These structures behave in essentially the same way as structs in C, subject to certain restrictions concerning self-references. Structure names and field names are subject to the same restrictions as normal identifiers. Since this means that a token may be considered an identifier or a type or a field identifier depending on its usage, this makes parsing GLSL as parsed by for example Google Chrome technically context-sensitive. 

However, the parser used, Jison, only supports LALR grammars. General context-free parsers are notoriously complicated and tend to have high time complexity. Given that the grammar is otherwise LALR, I deemed these algorithms to be far too inefficient.

The grammar as provided in the specification introduces extra tokens for structure and field names, skipping over the problem of context sensitivity entirely, but suggesting that the lexer should be able to decide. Therefore by way of an initial, straightforward implementation of struct parsing, I chose to make the parser simply append structure/field names to the appropriate part of the matching logic of the lexer whenever a new structure definition was encountered. This will fail to parse certain shader programs. for example ones where the name of a structure is also used as the name for a variable, or more likely, some field sharing the name of a variable. Since these programs are reasonably unusual, this was deemed to be acceptable for this project.

\clearpage
\section{Pipeline specification}
The purpose of the pipeline specification stage is to take input from the user specifying the shaders to be used and the connections between them --- that is, when a shader uses the framebuffer another shaderhas drawn to as an input texture. Note that at this point the terminology used is similar to that used by the \textsc{glow} project rather than the general OpenGL definition, in that the term `shader' is used to refer to the combination of a vertex and a fragment shader. While this may seem unorthodox, it is not generally useful to talk about individual vertex and fragment shaders in this project. Where the distinction is unclear, the term `shader program' is used to refer to an individual GLSL program.

For the actual pipeline specification, I could either develop my own specification language or reuse some existing language. Since this project uses WebGL, I decided to use JavaScript. This has the added bonus of enabling the user to specify a more dynamic pipeline. This is particularly useful given the slight variations in WebGL implementation between web browsers. For example, the pipeline could choose to use a slightly simpler version of a shader for Firefox, given the worse performance on that browser (in my personal experience), or perform some precomputation of values. This would also make the addition of some facility for animation simpler, since this could use some arbitrary JavaScript fragment executed between each render loop, using the same interface to change parameter values.

While I could have required pipeline specifications to produce an actual shader graph in its entirety, the objects used internally by my project are sufficiently complicated that I did not want to expose them directly to the user. Therefore, a simplified environment containing methods for creating dummy shaders is created. These dummy shaders can then be converted into the internal format later. In the interface as presented to the user, most of the internal complexity of a shader instance is hidden. As far as the user is concerned, an instance has parameters, a name and a size, but nothing else. Using the output of another shader instance uses the same interface as any other parameter. The actual behaviour, in which an FBO is written to by one instance and read from by another, is left implicit.

To construct this environment, the pipeline specification text (as JavaScript) is evaluated within a function containing the appropriate function definitions. These definitions include shorthand functions like \texttt{sampler2D()} for specifying parameters (in this case, a texture), and shorthand objects with convenient names like \texttt{gaussHShader} in Listing~\ref{pipe} for creating shader dummy objects. These objects merely contain the shader instance name, type --- that is, the name of the shader it will be an instance of --- the size of the framebuffer object to which it will write and any specified parameters. 

The resulting directed acyclic graph (or DAG) of dummy objects is then traversed, creating an appropriate graph of actual shader instances. In the initial implementation, this includes placing parameters into an object as needed by \textsc{glow}, attaching a reference to the appropriate shader and adding name and size information. To make later updates simpler, this was later modified such that parameters that reference another shader are considered separate --- these must have an actual parameter referencing the correct framebuffer object generated. \texttt{uniform} parameters are further separated from other qualifiers (\texttt{attribute} and \texttt{varying}). Later, this stage was also modified to also include early initialisation of \textsc{glow} objects. This is discussed in Section~\ref{opt}.

Listing~\ref{pipe} shows a simple example of a pipeline specification. This corresponds to the shader graph in Figure~\ref{pipe-graph}. The variable ``output'' is special, in that it corresponds to the highest node in the `tree' and therefore the final node in the pipeline --- the shader to be rendered to the screen. Notice that some parameters are left blank --- these can be provided later via the parameter UI. Individual shader instances can be given names for identification within the UI on initialisation, and the size of the FBO to which the shader is drawn can also be specified. When these are not specified, an identifier is generated based on the shader name and the size is assumed to be the size of the canvas. Unfortunately, since JavaScript lacks a way to dynamically find the variable names within a given context, it is not possible to automatically infer shader instance names without using a JavaScript parser, which is beyond the scope of this project (and would not always be able to infer names since shader instances can be anonymous).


\begin{listing}[H]
\begin{minted}{javascript}
output = new gaussHShader("output");
gaussv = new gaussVShader();
input = new textureShader("input",{width:800,height:600});

output.img = gaussv;
gaussv.img = input;
\end{minted}
\caption{Example pipeline specification.\label{pipe}}
\end{listing}

\begin{figure}

\begin{tikzpicture}[->,node distance=1cm, auto,]
\node[punkt] (input) {input};
\node[punkt, below=of input] (texture) {texture};
%edge[pil,->] (input.south);
\node[punkt, right=of input] (gaussv) {gaussv};
%edge[pil,<-] (input.east);
\node[punkt, right=of gaussv] (output) {output};
%edge[pil,<-] (gaussv.east);
\node[punkt, right=of output] (screen) {screen};
%edge[pil,<-] (output.east);

\node[punkt, below=of gaussv] (gaussv-sigma) {sigma};
%edge[pil] (gaussv.south);
\node[punkt, below=of output] (output-sigma) {sigma};
%edge[pil] (output.south);

\path[pil]   (texture) edge              node {img} (input)
        (input)   edge              node {img} (gaussv)
        (gaussv)  edge              node {img} (output)
        (output)  edge              node {} (screen)
 (gaussv-sigma)   edge              node {sigma (V)} (gaussv)
 (output-sigma)   edge              node {sigma (H)} (output);

\end{tikzpicture}
\caption{The shader graph for the pipeline specified in Listing~\ref{pipe}.\label{pipe-graph}}
\end{figure}

\clearpage
\section{Parameter UI generation}
\label{ui-params}
This stage takes a graph of shaders and creates the appropriate HTML user interface. For each parameter of the shader which needs specifying --- that is, is of type \texttt{uniform} and is not already specified --- it produces an appropriate set of input elements. For each parameter a label based upon either the provided name or a hopefully informative generated name is generated, along with a type-specific label and widget.

In the initial implementation, the appropriate shader values would be updated when a new render was requested. However this is inefficient, and makes the user interface less responsive. Later implementations therefore use a different method: each input element has associated event listeners that will update the value within the shader, correctly invalidate the relevant \textsc{glow} cache and request the preview be re-rendered. This is achieved cleanly thanks to the presence of closures in JavaScript: the UI generating function has access to the relevant \textsc{glow} object for the current parameters, and as such event handles attached within it also have access to the correct object, without the need for less efficient navigation of the shader graph. 

While simple text boxes provide complete control over the parameters, for certain types of parameter more useful widgets can be provided. The most obvious example is for \texttt{vec3} and \texttt{vec4} (vectors of length 3 and 4) parameters, which are often used to specify colours. For such parameters, I constructed a colour picking widget based on the web colour picker by Stefan Petre \citep{color}, as can be seen in Figure~\ref{colour}. 

\begin{figure}
\centering
\includegraphics[width=100mm]{colour.png}
\caption{Colour picker.\label{colour}}
\end{figure}

Another addition which would make use of the range parameter annotations (see Section~\ref{annotations}) would be sliders for numeric parameters. This was skipped for time reasons. Texture parameters assume sensible defaults rather than requiring the user to specify the verbose information required by \textsc{glow}. 

\subsection*{Struct handling}
Since \textsc{glow} expects structure parameters to be specified by ordinary JavaScript objects, which are also used to specify parameters, structs can be easily and simply handled by recursing on the parameter generation function. While for languages like C, this could produce an infinite loop if the structure references itself, GLSL does not permit this. However, the parser will not reject such a self-referential structure definition, which would cause infinite recursion. This is avoided using a counter set to a sufficiently large value that no valid GLSL program could reach it. This is possible since GLSL programs are required to compile to a sufficiently small amount of instructions, due to the limitations of stream processors.

\clearpage
\section{Pipeline generation}
\subsection*{Pipeline linearisation}
At this stage in the process, a directed, acyclic graph (DAG) of shader instances has been generated, with connections representing dependencies between instances --- that is, instances that write to textures that will be read by other instances. In order to actually render the DAG, it must be linearised it such that the node labeled ``output'' is last. This is the primary task of the pipeline generation stage, namely generating a list containing the nodes of the shader DAG. Since JavaScript uses pass-by-reference for objects, we can retain the original DAG at no extra cost.

This linearisation is achieved using a topological sort. The algorithm described by Cormen et al.\citep[chap. 22.4]{topsort} was used. The algorithm is shown in Listing~\ref{simpletopological}.
\begin{algorithm}
\begin{algorithmic}
\State $L \gets $ Empty list that will contain the sorted nodes
\State $S \gets $ Set of all nodes with no outgoing edges
\ForAll{node $n$ in $S$}
    \State \textsc{visit}($n$)
\EndFor 
\Function{visit}{node}
    \If{$n$ has not been visited yet}
        mark $n$ as visited
        \ForAll{node $m$ with an edge from $m$ to $n$}
            \State \textsc{visit}($m$)
        \EndFor
        \State add $n$ to $L$
    \EndIf
\EndFunction
\end{algorithmic}
\caption{Pseudocode for topological sort algorithm.\label{simpletopological}}
\end{algorithm}

Note that there is only one node in S, the output node, thus the initial for loop can be replaced with a single call. Additionally, all nodes must be marked as not visited before running this algorithm, as the linearisation procedure may be run multiple times. However, the algorithm shown in Listing~\ref{simpletopological} cannot detect when the graph contains a loop. This situation will only occur if the user inputs a shader graph containing cycles --- in such a case, the program should produce an error. This is achieved by modifying the visit function to take an extra argument, the list of all previously seen nodes on this path. At each step, if the current node is in the list of nodes, a cycle has been detected, and an error is thrown.

\subsection*{Pipeline initialisation}
\label{pipe-init}
In this stage, each shader object in the pipeline is assigned an actual \textsc{glow} shader. This is the object that contains the actual compiled shader that will be called at render-time. Initially, this was done in the most straightforward manner possible, by simply iterating through the pipeline and generating new \textsc{glow} objects after any modification. All parameter values would be retrieved from the UI at this point. This required the user to manually request that the pipeline is updated before rendering. 

Recreating all \textsc{glow} objects and reading in all parameter values after a single parameter change is very inefficient, especially since this process would run after every individual parameter change. Therefore this stage is avoided where possible by ensuring that changing parameters does not require a complete update --- \textsc{glow} objects are created before the parameter UI generation stage, with dummy parameter values, and pipeline initialisation is only run after a change to the pipeline (as discussed in Section~\ref{opt}).

\section{Rendering}
The rendering process itself does as little as possible, for efficiency reasons. I offloaded most of the work from the rendering stage to other stages. This is important for later extensions involving animation, where we want the render loop to run as quickly as possible. In my very first prototype, the rendering stage also performed \textsc{glow} object initialisation and parameter value retrieval. Now, the render loop simply iterates through the shader pipeline list, binding the framebuffer object associated with each shader, rendering the shader and then unbinding the FBO. The FBO binding is skipped for the final shader, which is instead rendered to the preview context. While the initial render function cleared the \textsc{glow} cache for each shader instance at the start of each call, we can avoid this as discussed in Section~\ref{glow-cache}.

\clearpage
\section{Optimisations}
\label{opt}
As stated in the introduction, one of the main aims of this project is to make the User Interface as interactive as possible. To be more specific, the delay between making some change and seeing the effect of this change should be as short as possible. Since changes on this timescale will usually be quite small, for example changing a single parameter, or modifying a single shader, by reusing existing state where possible this delay can be significantly reduced.

\subsection{Parameter update changes}
As discussed in Section~\ref{ui-params} and Section~\ref{pipe-init}, in the initial implementation all stages following Parameter UI Generation must be called, at the users request, following each change to a parameter. This includes a step in which every parameter in the UI is updated, initialisation of \textsc{glow} shader objects, and pipeline linearisation. This is obviously sub-optimal, both since all parameters must be updated and redundant steps be re-run, and since the user is required to request re-rendering, which reduces the level of interactivity.

The solution to this problem is briefly mentioned in Section~\ref{pipe-init}. Dummy parameters are obtained by modifying the parameter UI generation stage to generate a dummy value for each parameter as well as the actual UI. Complete \textsc{glow} object generation is now only carried out on startup, and after changes to the pipeline. Parameter changes now generate events which update the relevant \textsc{glow} object's parameters, and request a new render. The entire pipeline initialisation stage can now be skipped. Updates also invalidate the \textsc{glow} cache as discussed below.

This optimisation has added interactivity benefits in that the effect of a parameter can be seen as soon as it is added, without requiring the user to specify a value.

\subsection{Modified shader detection}
\label{msd}
The implementation as discussed above performs the entire shader parsing through pipeline initialisation process, throwing away all previous data, whenever a shader program is modified. Since only one shader program can be modified at once, this is sub-optimal. This is particularly important since we would like to be able to render a new preview of the pipeline output as quickly as possible. The most obvious optimisation here is to only update shader instances in the pipeline that use the modified shader. This is achieved by tagging shader instances with the name of their shader. Shader update calls then only need to attach new \textsc{glow} shaders to the appropriate shaders, using their existing parameters.

\subsection{Parameter updating}
The above discussion ignores the possibility of the presence of different parameters in the modified version of the shader. There are three problems here: the shader may require new parameters, it may require less, or a parameter's type may have changed. First, parameters whose types have changed or that are no longer required are removed from the parameter list and the UI. Next, as when generating the UI normally, the UI for the specific shader is generated, using the updated list of parameters. Since this will ignore parameters that are already specified, only widgets for new parameters are generated. This includes parameters whose types have changed, since these have been removed from the parameter list. Default parameters are generated for these new parameters during this process. Finally, the newly generated widgets can be added to the UI where appropriate. While it would be possible to keep parameter values for removed parameters in case they are later re-added, I deemed this to be too complicated to be worth the questionable benefits.

\subsection{\textsc{glow} cache}
\label{glow-cache}
For efficiency reasons, \textsc{glow} itself tries to cache as much as possible in GPU-accessible memory. In particular, parameter values are cached and updates to parameters must respect this and invalidate the cache before the next render call. In the initial implementation, the \textsc{glow} cache was cleared at the start of each render call. This is inefficient, and was soon modified such that the \textsc{glow} cache was only cleared after parameter changes and pipeline changes. However, these gains are mostly only noticeable during animation.

\subsection{FBO reuse}
In the initial implementation, framebuffer objects are simply discarded and new ones are allocated after each pipeline change. This texture allocation overhead could be avoided. However, since FBOs come in different sizes, we cannot naively allocate some 'pool' of available FBOs. This is achieved by simply keeping separate pools of already allocated FBOs for each size of FBO that has been used in the past. Shader updates using the event-based system described above can reuse the existing FBO for each instance so long as the pipeline structure does not change, this is an additional benefit of the optimisations discussed above.

\section{User interface}
\subsection{Editor}
As discussed in \ref{cmirror}, I chose to use the CodeMirror text editor to provide GLSL syntax highlighting to users. While CodeMirror does not provide GLSL highlighting, it does provide a general, 'C-like' highlighting mode, and simple hooks for custom parsers. Initially, I based my syntax highlighting on the 'C-like' mode provided. This was a simple matter of providing the correct keywords. However, this only provides simple highlighting. Given that each shader is parsed anyway, it would seem sensible to modify the parser used for parameter extraction to also provide syntax highlighting information. A particularly common feature provided by IDEs is highlighting the line on which a syntax error occurs. This was skipped for time reasons, but should only require a small amount of extra effort.

\subsection{Layout}
The initial layout of the UI was straightforward, consisting of a pair of text editors for each shader program, a text editor for the pipeline and parameter specification, the parameter UI, a sequence of control buttons and a preview box. This is shown in Figure~\ref{bad-ui}. While this layout was spartan and not user friendly, it was sufficient for initial testing, and allowed for easy debugging of separate stages.
\begin{figure}
\centering
\includegraphics[width=100mm]{UI-before.JPG}
\caption{Original Layout. Image of Tux with acknowledgements to lewing@isc.tamu.edu and The GIMP\label{bad-ui}}
\end{figure}


\subsection{Improved user interface}
The layout described above suffers from a number of usability problems. Firstly, the UI does not fit at all on an average-sized screen. This requires the user to be constantly scrolling to use it, adding a delay between the user making a change, and seeing its effect. 

Secondly, the control buttons are unintuitive, and the need for the user to use them whenever they wish to see the effect of a change further increases the expected length of a user's development cycle. To avoid requiring the user to request new updates, events were added which are triggered by changes to objects in the interface. Modifying shaders causes a shader update to be triggered, which, as described in Section~\ref{opt}, causes any relevant shader instances to be updated, along with their parameter widgets. Modifying the pipeline causes the pipeline to be regenerated, and changing shader parameters causes them to be updated as appropriate. All changes cause the pipeline output to be re-rendered. This resulted in a much more dynamic, interactive user interface.

A new layout was also designed, with inspiration taken from the typical construction of IDEs, where tiling is employed to maximise screen usage. From Figure~\ref{good-ui} we can see the improved layout. Shader programs occupy the centre of the screen, and can be switched between by clicking on the shader's name. The right hand side of the screen is occupied by the parameter specification UI, and the left hand side feature the shader pipeline output, along with the pipeline specification itself. Each pane can be individually hidden, except for the shader programs themselves. The need control buttons have been eliminated using the hooks developed in Section~\ref{opt}, but are provided in a collapsible frame at the bottom of the screen. There is a similar collapsible frame at the top for error output. Compared to the old layout, this provides:
\begin{itemize}
\item Minimal scrolling --- shaders/pipeline specification/parameters can be changed without having to scroll to see the result
\item Improved interactivity --- updated render provided as soon as possible, rather than on request
\item Improved usability --- no confusing buttons to press requiring knowledge of internal program workings 
\item Error feedback --- introduction of an error popup means users are informed of errors, but screen space is not wasted otherwise
\item Shader renaming --- shaders can be renamed by clicking on their editor's titles
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=150mm]{UI-after.JPG}
\caption{Improved Layout.\label{good-ui}}
\end{figure}

\cleardoublepage
\chapter{Evaluation}
Evaluation of this project chiefly consists of two parts, correctness and performance. Correctness evaluation is intended to demonstrate correct functioning of the core project. This consists of two parts: parser testing, to ensure parameters are correctly provided to the user interface, and output testing, to verify that the output of shader pipelines is visually as expected. Performance evaluation provides an analysis of the effect in terms of both performance and resource usage of some of the optimisations. In particular, the effect of avoiding unnecessary shader recompilation and later steps on performance is evaluated.

\section{Parser correctness}
In order to assist with development of the parser, and in particular to assist in regression testing, a test harness was developed for the testing the correctness of parameters returned by the parser. Since the parser does not access any browser-specific APIs, it was possible to perform this testing in a simple, automated way on the command line. Each test is specified by a shader program, and the result that should be returned by the parser --- the parameters and their associated information, any struct definitions, and whether the parser should fail to parse it and raise an exception, in the case of an invalid program. Parser output for each test specification is then compared against the actual output for a set of tests. These include correctly throwing/not raising an exception, returning the correct parameter names/a correct subset of the parameter names, all/some of the correct parameter types, the correct struct definitions, and so on. An additional script was written to perform these tests over all git revisions. 

In Listing~\ref{verbose} we see a small sample of the complete test results for an individual version, verbatim from actual command line output. The second column lists individual tests --- here, we see some very basic tests verifying that for example \texttt{uniform int} is parsed correctly. The third column lists the sub-tests for each test: these are identical for each test, with the exception of tests that are intended to fail to parse. The right column shows whether the sub-test is passed. We can see that for the \texttt{varying lowp vec4}, the parameter types are incorrect for this git revision. Listing~\ref{summary} shows a sample of the test summary for some of the later git revisions. Each line represents a different git revision. Note that there are two sub-tests consistently failed: these correspond to tests testing correct struct parsing for structs sharing names with variables or fields, which cannot be parsed with the current parser implementation, as discussed in Section~\ref{struct-parse}.
\begin{listing}[H]
{
\begin{verbatim}
Module  Test        Assertion                       Result

glsl    uint        Successfully parses             ok

                    One correct parameter           ok

                    Correct parameter names         ok

                    Correct parameter names only    ok

                    Correct parameters              ok

                    Correct structs                 ok

        ufloat      Successfully parses             ok

                    One correct parameter           ok

                    Correct parameter names         ok

                    Correct parameter names only    ok

                    Correct parameters              ok

                    Correct structs                 ok

        vlvec4      Successfully parses             ok

                    One correct parameter           ok

                    Correct parameter names         ok

                    Correct parameter names only    fail

                    Correct parameters              fail

                    Correct structs                 ok

\end{verbatim}
}
\caption{Sample of verbose test results.\label{verbose}}
\end{listing}
\begin{listing}[H]
{\small
\begin{verbatim}
Summary:

File                   Failed    Passed    Total     Runtime

shomp/parser/glsl.js   2         34        36        177

shomp/parser/glsl.js   2         34        36        179

shomp/parser/glsl.js   2         34        36        179

shomp/parser/glsl.js   2         34        36        173

shomp/parser/glsl.js   2         34        36        175

shomp/parser/glsl.js   2         34        36        178


\end{verbatim}
}
\caption{Sample of test summary.\label{summary}}
\end{listing}
In Section~\ref{parser-git} we see a graph showing the percentage of tests correctly completed, from the first revision including a parser. The notable jumps in tests passed correspond to implementation of the currently used parameter format, scoping corrections (i.e. discarding parameters hidden within functions) and structure support. Not all tests are passed at the final version --- these include tests that contain structure definitions and variables with the same name, the parsing of which is not possible with my current parser, as discussed in Section~\ref{struct-parse}. Also tests that rely on preprocessor commands will fail since a preprocessor was not implemented.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[xlabel={git revision},ylabel={tests passed}]
\addplot file {parser.data};
\end{axis}
\end{tikzpicture}
\caption{Percentage of parser tests passed vs. git revision.\label{parser-git}}
\end{figure}

\section{Pipeline correctness}
In order to demonstrate visually correct shader pipeline output, a small number of example pipelines were constructed. One of these examples were constructed myself, and one is loosely based upon existing WebGL code\cite{reaction-diffusion}, modifying them to work within my project. Output from the modified version can then be compared with the original. These example programs use a few different shaders connected together to produce a single visual effect. Below we can see samples of the output of my implementations.

\subsection*{Gaussian blur}
Gaussian blurring of an image can be naturally divided into two stages, a horizontal and a vertical blur. A simple pipeline is shown in Listing~\ref{gauss-pipe}, and its graph in Figure~\ref{gauss-graph}. Parameter values have been mostly omitted in the pipeline for clarity. The two stages each perform orthogonal one-dimensional Gaussian blurs, defined by $G(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}}$. This is equivalent to a two-dimensional blur as defined by $G(x,y) = \frac{1}{2\pi \sigma^2} e^{-\frac{x^2 + y^2}{2 \sigma^2}}$. The first shader, ``input'' is strictly unnecessary, and is only for illustration purposes. Figure~\ref{gauss-img} shows a comparison of the original to the blurred image.

\begin{listing}[H]
\begin{minted}{javascript}
output = new gaussHShader("output");
gaussv = new gaussVShader("gaussv");
input = new textureShader("input",{width:800,height:600});

output.img = gaussv;
gaussv.img = input;
\end{minted}
\caption{Gaussian blur pipeline.\label{gauss-pipe}}
\end{listing}

\begin{figure}

\begin{tikzpicture}[->,node distance=1cm, auto,]
\node[punkt] (input) {input};
\node[punkt, below=of input] (texture) {texture};
%edge[pil,->] (input.south);
\node[punkt, right=of input] (gaussv) {gaussv};
%edge[pil,<-] (input.east);
\node[punkt, right=of gaussv] (output) {output};
%edge[pil,<-] (gaussv.east);
\node[punkt, right=of output] (screen) {screen};
%edge[pil,<-] (output.east);

\node[punkt, below=of gaussv] (gaussv-sigma) {sigma};
%edge[pil] (gaussv.south);
\node[punkt, below=of output] (output-sigma) {sigma};
%edge[pil] (output.south);

\path   (texture) edge              node {img} (input)
        (input)   edge              node {img} (gaussv)
        (gaussv)  edge              node {img} (output)
        (output)  edge              node {} (screen)
 (gaussv-sigma)   edge              node {sigma (V)} (gaussv)
 (output-sigma)   edge              node {sigma (H)} (output);

\end{tikzpicture}
\caption{Corresponding shader graph for Listing~\ref{gauss-pipe}.\label{gauss-graph}}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=60mm]{tux-blurred.JPG}
\caption{Comparison of original to blurred image.\label{gauss-img}}
\end{figure}

\subsection*{Sanity test}
This example was developed in order to ensure that long pipelines are rendered correctly. It uses a long chain of only one type of shader. These shaders merely increment the value of the red channel, so a chain of length n should increment the initial red value by n times the step size. A simplified version of the pipeline is shown in Listing~\ref{sanity-pipe}. This example demonstrates how using an existing programming language for pipeline specification allows concise construction of more complex pipelines. In Figure~\ref{sanity-img} we see the output of this shader applied to an image of Tux, with a step size of 0.1. As expected, the black regions of the image have become completely red.

\begin{listing}[H]
\begin{minted}{javascript}
output = new step(); 
var c = output;
for (var i=0;i<10;i++) {
  c.img = new step();
  c = c.img;
}
c.img = sampler2D('tux.JPG');
\end{minted}
\caption{Sanity test pipeline.\label{sanity-pipe}}
\end{listing}

\begin{figure}
\centering
\includegraphics[height=60mm]{red-tux.png}
\caption{Sanity test output.\label{sanity-img}}
\end{figure}

\section{Performance}
\subsubsection{Effect of modified shader detection}
In Section~\ref{opt}, we discussed the modification of the pipeline initialisation process to avoid unnecessary shader re-parsing, compilation, and pipeline generation after modification of a shader. In order to compare the performance of the updating procedure before and after these optimisations were introduced, a custom test harness was developed. Since the existing test harnesses for JavaScript that provide performance testing are mostly quite heavyweight, being intended for large-scale website testing, I implemented this from scratch.

Each test specification consists of a set of shaders, and a shader graph specification in which all necessary parameters are specified. Once a shader pipeline has been generated for a given specification, a shader update event is generated for a particular shader, and its completion timed. This is repeated 10 times to obtain an average and standard deviation. Similarly the completion of a complete re-generation of the entire shader pipeline is timed 10 times. WebGL is intelligent enough to avoid recompiling identical shader text, meaning that repeated runs of the same test would give smaller times for the later tests. I avoided this by padding shaders programs with a different amount of trailing whitespace between iterations. This is sufficient to defeat the caching behaviour without changing the semantic meaning of the shader program. This is repeated for each shader in the test specification. From Figure~\ref{msd-graph} we can see the result of these tests for a particular test specification. The error bars here signify the standard deviation over all trials. It is clear from this graph that these optimisations have a significant effect on the speed of an update. While for smaller examples like ``simple'', which consists only a single shader, the difference in performance is unlikely to be noticed, for a more complicated example the benefit is clear.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    enlargelimits=0.15,
    legend style={at={(0.5,-0.15)},
      anchor=north,legend columns=-1},
    ylabel={avarage time (ms)},
    xtick=data,
    %nodes near coords,
    nodes near coords align={vertical},
symbolic x coords={simple,gauss,chain,sanity,redundant,ham}]
\addplot+[error bars/.cd,y dir=both,y explicit] coordinates {(simple,17.4) +- (2.7640549922170585,2.7640549922170585)
(gauss,45.1) +- (3.448187929913315,3.448187929913315)
(chain,76.6) +- (5.102940328869315,5.102940328869315)
(sanity,81.6) +- (8.662563131083136,8.662563131083136)
(redundant,20.6) +- (3.1048349392519934,3.1048349392519934)
};
\addplot+[error bars/.cd,y dir=both,y explicit] coordinates {(simple,12) +- (1.1832159566199256,1.1832159566199256)
(gauss,25) +- (1.7888543819998444,1.7888543819998444)
(chain,46.2) +- (7.48064168370602,7.48064168370602)
(sanity,50.5) +- (9.810708435174291,9.810708435174291)
(redundant,3.3) +- (5.0408332644514235,3.3)
};
\legend{unoptimised, optimised}
\end{axis}
\end{tikzpicture}
\caption{Graph comparing shader update times pre- and post-optimisation.\label{msd-graph}}
\end{figure}

\cleardoublepage
\chapter{Conclusion}

\section{Main Results}
The project has been successful. The core of the project has been completed and functions correctly, and with the addition of the various optimisations and extensions, is sufficiently interactive to be useful. I have made the project source code available to others via GitHub, and I hope that it will prove useful. In Table~\ref{core_t} we can see that the core criteria were completed, and from Table~\ref{ext_t} we can see that a significant number of the extensions were completed. Of the extensions not implemented, two of them were deemed unsuitable. Texture unification as originally envisaged is supported trivially by the pipeline specification stage, and code separation would be sufficiently complex to be outside the scope of this project, and is unlikely to be worth the extra effort. In addition, a number of optimisations were made not listed in the table below.

\begin{table}[h!]
\centering
\begin{tabular}{p{9cm}l@{\hspace{0.25em}}}
\toprule
\textbf{Criterion} & \textbf{Successful?} \\
\midrule\arrayrulecolor{lightgray}
Interface to construct shaders, specify connections between shaders & Yes \\\midrule
Detection of parameters of set of shaders, parameters presented to the user somehow & Yes \\\midrule
Sample scene output for single shader & Yes \\\midrule
Demonstration of correct composition (with sample scene output) for various shaders with pipeline specifications & Yes\\\midrule
Provide basic syntax highlighting of GLSL in interface & Yes \\\arrayrulecolor{black}\bottomrule
\end{tabular}
\caption{Core Criteria.\label{core_t}}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{p{9cm}l@{\hspace{0.25em}}}
\toprule
\textbf{Criterion} & \textbf{Successful?} \\
\midrule\arrayrulecolor{lightgray}
Reuse of FBOs in shader pipeline & Yes \\\midrule
Automatic unification of identical textures & No \\\midrule
Splitting shared code off into separate shaders & No \\\midrule
Avoiding recompiling shaders unnecessarily & Yes \\\midrule
Improved UI based on annotations & Yes\\\midrule
Some facility for animation & No \\\midrule
Shader and/or scene test suite & No \\\arrayrulecolor{black}\bottomrule
\end{tabular}
\caption{Extensions.\label{ext_t}}
\end{table}
\section{Lessons learned}
The combination of WebGL and Linux is still a little unstable at times, and as such I would be less inclined to rely so much on a very recent technology for future projects.

While I was initially apprehensive about using JavaScript given it's reputation as an ugly language to work with and my experience of Internet Explorer 6-era JavaScript, I was pleasantly surprised by it's novel object model and quite functional underpinnings. Particularly useful was the ability to treat objects much like associative arrays. Combined with a good set of libraries like jQuery, I found modern JavaScript almost as convenient to work with as languages like Python or Ruby.

\section{Further work}
As this project was only partially focused on the development of a user interface, the current UI is fairly basic. Further work could be done to make the UI more usable, and possibly to conduct a user study to test its effectiveness. More sophisticated widgets would be a good start. A natural choice would be sliders, since the annotations are already present to support this. \textsc{glow} provides a nice interface for specifying rotation matrices that could be exposed. Error handling is currently quite basic, and could be improved. The extensions listed in Table~\ref{ext_t} were dropped for time reasons, but could also be incorporated with not too much additional effort. A number of techniques are used for dealing with context sensitivity in C without using a general context-sensitive parsing algorithm, which could potentially be applied to parsing GLSL with structs correctly.

Since beginning this project, \textsc{glow} has received numerous bug fixes and backwards-incompatible updates. Porting my project to the new version should --- hopefully --- be quite straightforward and may fix some of the bugs. 

Both the specification and implementation of WebGL have changed significantly since their first proposal, and I expect them to continue to do so for some time yet. As such further work may be necessary in the future as these change. Of particular note is the current lack of multiple render targets in current implementations \citep{webgl-future}. If in the future implementations begin to support multiple render targets, adding support for these to my project will be a moderately non-trivial task.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

%\chapter{Latex source}

%\section{diss.tex}
%{\scriptsize\verbatiminput{diss.tex}}

%\section{proposal.tex}
%{\scriptsize\verbatiminput{proposal.tex}}

%\section{propbody.tex}
%{\scriptsize\verbatiminput{propbody.tex}}



%\cleardoublepage

%\chapter{Makefile}

%\section{\label{makefile}Makefile}
%{\scriptsize\verbatiminput{makefile.txt}}

%\section{refs.bib}
%{\scriptsize\verbatiminput{refs.bib}}


\cleardoublepage

\chapter{Project Proposal}

%\input{propbody}

\clearpage

\chapter{Parser source}


\scriptsize\verbatiminput{glsl.jison}


\end{document}
